{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functools import partial\n",
    "from ipywidgets import interact\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "N_TRAIN = 10_000\n",
    "N_SAMPLES = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasks.toy_examples.data_generators import SBIGaussian2d\n",
    "# Task\n",
    "task = SBIGaussian2d(prior_type=\"gaussian\")\n",
    "# Prior and Simulator\n",
    "prior = task.prior\n",
    "simulator = task.simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nse import NSE, NSELoss\n",
    "from sm_utils import train\n",
    "\n",
    "# Train data\n",
    "theta_train = task.prior.sample((N_TRAIN,))\n",
    "x_train = simulator(theta_train)\n",
    "\n",
    "# normalize theta\n",
    "theta_train_ = (theta_train - theta_train.mean(axis=0)) / theta_train.std(axis=0)\n",
    "\n",
    "# normalize x\n",
    "x_train_ = (x_train - x_train.mean(axis=0)) / x_train.std(axis=0)\n",
    "\n",
    "# score_network\n",
    "dataset = torch.utils.data.TensorDataset(theta_train_.cuda(), x_train_.cuda())\n",
    "score_net = NSE(theta_dim=2, x_dim=2, hidden_features=[128, 256, 128]).cuda()\n",
    "\n",
    "# avg_score_net = train(\n",
    "#     model=score_net,\n",
    "#     dataset=dataset,\n",
    "#     loss_fn=NSELoss(score_net),\n",
    "#     n_epochs=200,\n",
    "#     lr=1e-3,\n",
    "#     batch_size=256,\n",
    "#     prior_score=False, # learn the prior score via the classifier-free guidance approach\n",
    "# )\n",
    "# score_net = avg_score_net.module\n",
    "# torch.save(score_net, \"score_net.pkl\")\n",
    "# torch.save(score_net, \"score_net_normal_prior.pkl\")\n",
    "# score_net = torch.load(\"score_net_normal_prior.pkl\")\n",
    "score_net = torch.load(\"../score_net.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/parietal/store3/work/jlinhart/miniconda3/envs/diff4sbi/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# meshgrid for theta space\n",
    "theta1 = torch.linspace(-10, 10, 100)\n",
    "theta2 = torch.linspace(-10, 10, 100)\n",
    "\n",
    "theta1_, theta2_ = torch.meshgrid(theta1, theta2)\n",
    "theta_ = torch.stack([theta1_.reshape(-1), theta2_.reshape(-1)], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores for $n=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d0865be81b4280b41266df973e5ee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='t', max=1.0, step=0.01), Output()), _dom_classes=('w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute learned and analytic scores\n",
    "from vp_diffused_priors import get_vpdiff_gaussian_score\n",
    "\n",
    "# observation\n",
    "theta_true = torch.FloatTensor([-5, 150])  # true parameters\n",
    "x_obs = simulator(theta_true)  # x_0 ~ simulator(theta_true)\n",
    "x_obs_ = (x_obs - x_train.mean(axis=0)) / x_train.std(axis=0)\n",
    "\n",
    "# true posterior\n",
    "true_posterior = task.true_posterior(x_obs)\n",
    "# rescale true posterior\n",
    "loc = (true_posterior.loc - theta_train.mean(axis=0)) / theta_train.std(axis=0)\n",
    "cov = (\n",
    "    torch.diag(1 / theta_train.std(axis=0))\n",
    "    @ true_posterior.covariance_matrix\n",
    "    @ torch.diag(1 / theta_train.std(axis=0))\n",
    ")\n",
    "true_posterior_ = torch.distributions.MultivariateNormal(loc=loc, covariance_matrix=cov)\n",
    "\n",
    "# plot scores diff on meshgrid\n",
    "def mesh_score_diff(t):\n",
    "    t = torch.tensor(t)\n",
    "    score_ana = get_vpdiff_gaussian_score(true_posterior_.loc, true_posterior_.covariance_matrix, score_net)(theta_, t=t)\n",
    "    score_learned = score_net.score(theta_.cuda(), x_obs_.cuda(), t=t.cuda()).detach().cpu()\n",
    "\n",
    "    norm_score_ana = torch.linalg.norm(score_ana, dim=-1).reshape(100, 100)\n",
    "    norm_score_learned = torch.linalg.norm(score_learned, dim=-1).reshape(100, 100)\n",
    "    diff_score = torch.linalg.norm(score_learned - score_ana, dim=-1).reshape(100, 100)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "    ax[0].contourf(theta1_, theta2_, norm_score_learned, levels=100)\n",
    "    ax[0].set_title(f\"Norm learned score (t={t:.2f})\")\n",
    "    ax[1].contourf(theta1_, theta2_, norm_score_ana, levels=100)\n",
    "    ax[1].set_title(f\"Norm analytic score (t={t:.2f})\")\n",
    "    ax[2].contourf(theta1_, theta2_, diff_score, levels=100)\n",
    "    ax[2].set_title(f\"Norm difference score (t={t:.2f})\")\n",
    "\n",
    "    # add colorbar\n",
    "    for i, s in zip(range(3), [norm_score_learned, norm_score_ana, diff_score]):\n",
    "        fig.colorbar(ax[i].contourf(theta1_, theta2_, s, levels=100))\n",
    "\n",
    "interact(mesh_score_diff, t=(0, 1, 0.01));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gradlogL for $n\\geq1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da733b6d028e4dfda463c220087253e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='t', max=1.0, step=0.01), IntSlider(value=50, descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute gradlogL\n",
    "from debug_learned_gaussian import diffused_tall_posterior_score\n",
    "\n",
    "# observations\n",
    "x_obs_100 = torch.cat(\n",
    "    [simulator(theta_true).reshape(1, -1) for i in range(100)], dim=0\n",
    ")\n",
    "x_obs_100_ = (x_obs_100 - x_train.mean(axis=0)) / x_train.std(axis=0)\n",
    "\n",
    "# normalized prior\n",
    "loc_ = (prior.prior.loc - theta_train.mean(axis=0)) / theta_train.std(axis=0)\n",
    "cov_ = (\n",
    "    torch.diag(1 / theta_train.std(axis=0))\n",
    "    @ prior.prior.covariance_matrix\n",
    "    @ torch.diag(1 / theta_train.std(axis=0))\n",
    ")\n",
    "prior_ = torch.distributions.MultivariateNormal(loc=loc_, covariance_matrix=cov_)\n",
    "\n",
    "get_score_fn = lambda n_obs: partial(\n",
    "    diffused_tall_posterior_score,\n",
    "    prior=prior_, # normalized prior\n",
    "    posterior_fn_ana=task.true_posterior, # analytical posterior\n",
    "    x_obs=x_obs_100[:n_obs].cuda(), # observations\n",
    "    x_obs_=x_obs_100_[:n_obs].cuda(), # normalized observations\n",
    "    nse=score_net, # trained score network\n",
    "    # mean and std to normalize the analytical posterior\n",
    "    theta_mean=theta_train.mean(axis=0),\n",
    "    theta_std=theta_train.std(axis=0),\n",
    ")\n",
    "\n",
    "def mesh_gradlogL(t, n_obs):\n",
    "    t = torch.tensor(t)\n",
    "\n",
    "    # score function for tall posterior (learned and analytical)\n",
    "    score_fn = get_score_fn(n_obs)\n",
    "\n",
    "    out = score_fn(theta_.cuda(), t.cuda(), debug=True)\n",
    "    gradlogL, gradlogL_ana = out[1], out[5]\n",
    "    norm_gradlogL = torch.linalg.norm(gradlogL, dim=-1).reshape(100, 100)\n",
    "    norm_gradlogL_ana = torch.linalg.norm(gradlogL_ana, dim=-1).reshape(100, 100)\n",
    "    diff_gradlogL = torch.linalg.norm(gradlogL - gradlogL_ana, dim=-1).reshape(100, 100)\n",
    "\n",
    "    # plot all quantities next to each other\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "    axes[0].contourf(theta1_, theta2_, norm_gradlogL, levels=100)\n",
    "    axes[0].set_xlabel(\"theta_1\")\n",
    "    axes[0].set_ylabel(\"theta_2\")\n",
    "    axes[0].set_title(f\"Norm gradlogL (t={t})\")\n",
    "    axes[1].contourf(theta1_, theta2_, norm_gradlogL_ana, levels=100)\n",
    "    axes[1].set_xlabel(\"theta_1\")\n",
    "    axes[1].set_ylabel(\"theta_2\")\n",
    "    axes[1].set_title(f\"Norm gradlogL_ana (t={t})\")\n",
    "    axes[2].contourf(theta1_, theta2_, diff_gradlogL, levels=100)\n",
    "    axes[2].set_xlabel(\"theta_1\")\n",
    "    axes[2].set_ylabel(\"theta_2\")\n",
    "    axes[2].set_title(f\"Norm diff gradlogL/ana (t={t})\")\n",
    "\n",
    "    # add colorbars\n",
    "    for i, g in zip(range(3), [norm_gradlogL, norm_gradlogL_ana, diff_gradlogL]):\n",
    "        plt.colorbar(axes[i].contourf(theta1_, theta2_, g, levels=100))\n",
    "\n",
    "interact(mesh_gradlogL, t=(0, 1, 0.01), n_obs=(1, 100, 1)); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "241bd9458de94357b2cf81850bce8154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='t', max=1.0, step=0.01), Output()), _dom_classes=('w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30511c6a557742f7895b4743e33697d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=25, description='n_obs', max=50, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot max amplitude of gradlogL as a function of n_obs\n",
    "\n",
    "def plot_max_gradlogL_n(t):\n",
    "    t = torch.tensor(t)\n",
    "\n",
    "    max_gradlogL = []\n",
    "    max_gradlogL_ana = []\n",
    "\n",
    "    n_list = [2, 5, 10, 20, 50]\n",
    "    for n_obs in n_list: \n",
    "        # score function for tall posterior (learned and analytical)\n",
    "        score_fn = get_score_fn(n_obs)\n",
    "\n",
    "        out = score_fn(theta_.cuda(), t.cuda(), debug=True)\n",
    "        gradlogL, gradlogL_ana = out[1], out[5]\n",
    "        norm_gradlogL = torch.linalg.norm(gradlogL, dim=-1).reshape(100, 100)\n",
    "        norm_gradlogL_ana = torch.linalg.norm(gradlogL_ana, dim=-1).reshape(100, 100)\n",
    "\n",
    "        max_gradlogL.append(norm_gradlogL.max())\n",
    "        max_gradlogL_ana.append(norm_gradlogL_ana.max())  \n",
    "\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10, 5))\n",
    "    ax[0].plot(n_list, max_gradlogL)\n",
    "    ax[0].set_xlabel(\"n_obs\")\n",
    "    ax[0].set_ylabel(\"max gradlogL\")\n",
    "    ax[1].plot(n_list, max_gradlogL_ana)\n",
    "    ax[1].set_xlabel(\"n_obs\")\n",
    "    ax[1].set_ylabel(\"max gradlogL_ana\")\n",
    "\n",
    "def plot_max_gradlogL_t(n_obs):\n",
    "\n",
    "    max_gradlogL = []\n",
    "    max_gradlogL_ana = []\n",
    "\n",
    "    t_list = [0,0.1, 0.3, 0.5, 0.7, 0.9,1]\n",
    "    for t in t_list: \n",
    "        t = torch.tensor(t)\n",
    "        # score function for tall posterior (learned and analytical)\n",
    "        score_fn = get_score_fn(n_obs)\n",
    "\n",
    "        out = score_fn(theta_.cuda(), t.cuda(), debug=True)\n",
    "        gradlogL, gradlogL_ana = out[1], out[5]\n",
    "        norm_gradlogL = torch.linalg.norm(gradlogL, dim=-1).reshape(100, 100)\n",
    "        norm_gradlogL_ana = torch.linalg.norm(gradlogL_ana, dim=-1).reshape(100, 100)\n",
    "\n",
    "        max_gradlogL.append(norm_gradlogL.max())\n",
    "        max_gradlogL_ana.append(norm_gradlogL_ana.max())  \n",
    "\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10, 5))\n",
    "    ax[0].plot(t_list, max_gradlogL)\n",
    "    ax[0].set_xlabel(\"n_obs\")\n",
    "    ax[0].set_ylabel(\"max gradlogL\")\n",
    "    ax[1].plot(t_list, max_gradlogL_ana)\n",
    "    ax[1].set_xlabel(\"n_obs\")\n",
    "    ax[1].set_ylabel(\"max gradlogL_ana\")\n",
    "\n",
    "interact(plot_max_gradlogL_n, t=(0, 1, 0.01));\n",
    "\n",
    "interact(plot_max_gradlogL_t, n_obs=(1, 50, 1)); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff4sbi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
